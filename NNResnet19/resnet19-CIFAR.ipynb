{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for data augmentation and normalization\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learningRate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,in_features=64,out_features=64,stride=[1,1],down_sample=False):\n",
    "        # stride : list \n",
    "        # the value at corresponding indices are the strides of corresponding layers in a residual block\n",
    "        \n",
    "        super(BasicBlock,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_features,out_features,3,stride[0],padding=1,bias=False) #weight layer\n",
    "        self.bn1 = nn.BatchNorm2d(out_features) #weight layer\n",
    "        \n",
    "        self.relu = nn.ReLU(True) #relu\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_features,out_features,3,stride[1],padding=1,bias=False) #weight layer\n",
    "        self.bn2 = nn.BatchNorm2d(out_features) #weight layer\n",
    "\n",
    "        self.down_sample = down_sample\n",
    "        if down_sample:\n",
    "            self.downsample = nn.Sequential(\n",
    "                    nn.Conv2d(in_features,out_features,1,2,bias=False),\n",
    "                    nn.BatchNorm2d(out_features)\n",
    "                )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x0=x.clone()\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.down_sample:\n",
    "            x0 = self.downsample(x0)  \n",
    "        x = x + x0    # F(x)+x\n",
    "        x= self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels=3,num_residual_block=[3,4,6,3],num_class=1000,block_type='normal'):\n",
    "        super(ResNet,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,64,7,2,3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.maxpool = nn.MaxPool2d(3,2,1)\n",
    "\n",
    "        # if block_type.lower() == 'bottleneck':    \n",
    "        #     self.resnet,outchannels = self.__bottlenecks(num_residual_block)\n",
    "        # else:\n",
    "        self.resnet,outchannels = self.set_layers(num_residual_block)\n",
    "    \n",
    "        #extra layer for 19\n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(in_features=outchannels,out_features=num_class,bias=True)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.resnet(x)\n",
    "        #print(\"Before Last layer: \",x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        #print(\"After Last layer: \",x.shape)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x \n",
    "    \n",
    "    def set_layers(self,num_residual_block):\n",
    "        layer=[]\n",
    "        layer += [BasicBlock()]*num_residual_block[0]\n",
    "        inchannels=64\n",
    "        for numOFlayers in num_residual_block[1:]:\n",
    "            stride = [2,1] #updating the stride, the first layer of residual block\n",
    "            # will have a stride of two and the 2nd layer of the residual block have \n",
    "            # a stride of 1\n",
    "            downsample=True\n",
    "            outchannels = inchannels*2\n",
    "            for _ in range(numOFlayers):\n",
    "                layer.append(BasicBlock(inchannels,outchannels,stride,down_sample=downsample))\n",
    "                inchannels = outchannels\n",
    "                downsample = False \n",
    "                stride=[1,1]\n",
    "            \n",
    "        return nn.Sequential(*layer),outchannels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  resnet18(**kwargs):\n",
    "    return ResNet(num_residual_block=[2,2,2,2],**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (resnet): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model18 = resnet18()\n",
    "model18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model18.parameters(), lr=learningRate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_Loss = 0\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:  # Prints every 50 mini-batches\n",
    "            print(f'Batch {i + 1}, Loss: {running_loss / 50:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch_Loss = running_loss / 256\n",
    "    print(f'TOTAL EPOCH LOSS: {epoch_Loss}')\n",
    "    return epoch_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = 0\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    test_accuracy = accuracy\n",
    "    print(f'Accuracy on the test set: {100 * accuracy:.2f}%')\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcospagnoletti-ms\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\marco\\Documents\\vscode\\Python\\NNResnet19\\wandb\\run-20231213_104308-8y4ipohk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcospagnoletti-ms/ResNet19-CIFAR10-test/runs/8y4ipohk' target=\"_blank\">wandering-shape-23</a></strong> to <a href='https://wandb.ai/marcospagnoletti-ms/ResNet19-CIFAR10-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcospagnoletti-ms/ResNet19-CIFAR10-test' target=\"_blank\">https://wandb.ai/marcospagnoletti-ms/ResNet19-CIFAR10-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcospagnoletti-ms/ResNet19-CIFAR10-test/runs/8y4ipohk' target=\"_blank\">https://wandb.ai/marcospagnoletti-ms/ResNet19-CIFAR10-test/runs/8y4ipohk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/marcospagnoletti-ms/ResNet19-CIFAR10-test/runs/8y4ipohk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1cbc1995bd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "# set the wandb project where this run will be logged\n",
    "project=\"ResNet19-CIFAR10-test\",\n",
    "\n",
    "# track hyperparameters and run metadata\n",
    "config={\n",
    "\"learning_rate\": 0.01,\n",
    "\"architecture\": \"RESNET18 +1 layer\",\n",
    "\"dataset\": \"CIFAR-10\",\n",
    "\"epochs\": 30,\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING DEVICE:  cuda\n",
      "\n",
      "Epoch:  1\n",
      "Batch 50, Loss: 2.637\n",
      "Batch 100, Loss: 1.596\n",
      "Batch 150, Loss: 1.471\n",
      "TOTAL EPOCH LOSS: 0.2513175210915506\n",
      "Accuracy on the test set: 50.14%\n",
      "LR:  0.01\n",
      "\n",
      "Epoch:  2\n",
      "Batch 50, Loss: 1.312\n",
      "Batch 100, Loss: 1.252\n",
      "Batch 150, Loss: 1.198\n",
      "TOTAL EPOCH LOSS: 0.21063700481317937\n",
      "Accuracy on the test set: 56.03%\n",
      "LR:  0.01\n",
      "\n",
      "Epoch:  3\n",
      "Batch 50, Loss: 1.101\n",
      "Batch 100, Loss: 1.071\n",
      "Batch 150, Loss: 1.050\n",
      "TOTAL EPOCH LOSS: 0.17976200417615473\n",
      "Accuracy on the test set: 65.29%\n",
      "LR:  0.01\n",
      "\n",
      "Epoch:  4\n",
      "Batch 50, Loss: 0.996\n",
      "Batch 100, Loss: 0.953\n",
      "Batch 150, Loss: 0.933\n",
      "TOTAL EPOCH LOSS: 0.16598937311209738\n",
      "Accuracy on the test set: 67.95%\n",
      "LR:  0.01\n",
      "\n",
      "Epoch:  5\n",
      "Batch 50, Loss: 0.875\n",
      "Batch 100, Loss: 0.873\n",
      "Batch 150, Loss: 0.876\n",
      "TOTAL EPOCH LOSS: 0.15301166661083698\n",
      "Accuracy on the test set: 69.26%\n",
      "LR:  0.01\n",
      "\n",
      "Epoch:  6\n",
      "Batch 50, Loss: 0.833\n",
      "Batch 100, Loss: 0.811\n",
      "Batch 150, Loss: 0.814\n",
      "TOTAL EPOCH LOSS: 0.14166647335514426\n",
      "Accuracy on the test set: 70.24%\n",
      "LR:  0.01\n",
      "\n",
      "Epoch:  7\n",
      "Batch 50, Loss: 0.773\n",
      "Batch 100, Loss: 0.762\n",
      "Batch 150, Loss: 0.774\n",
      "TOTAL EPOCH LOSS: 0.13531050295569003\n",
      "Accuracy on the test set: 72.41%\n",
      "LR:  0.01\n",
      "\n",
      "Epoch:  8\n",
      "Batch 50, Loss: 0.757\n",
      "Batch 100, Loss: 0.721\n",
      "Batch 150, Loss: 0.725\n",
      "TOTAL EPOCH LOSS: 0.13047546846792102\n",
      "Accuracy on the test set: 72.36%\n",
      "LR:  0.01\n",
      "\n",
      "Epoch:  9\n",
      "Batch 50, Loss: 0.694\n",
      "Batch 100, Loss: 0.729\n",
      "Batch 150, Loss: 0.685\n",
      "TOTAL EPOCH LOSS: 0.1250749269966036\n",
      "Accuracy on the test set: 74.38%\n",
      "LR:  0.01\n",
      "\n",
      "Epoch:  10\n",
      "Batch 50, Loss: 0.656\n",
      "Batch 100, Loss: 0.666\n",
      "Batch 150, Loss: 0.679\n",
      "TOTAL EPOCH LOSS: 0.1169269869569689\n",
      "Accuracy on the test set: 75.56%\n",
      "LR:  0.005\n",
      "\n",
      "Epoch:  11\n",
      "Batch 50, Loss: 0.595\n",
      "Batch 100, Loss: 0.582\n",
      "Batch 150, Loss: 0.582\n",
      "TOTAL EPOCH LOSS: 0.10439076623879373\n",
      "Accuracy on the test set: 79.28%\n",
      "LR:  0.005\n",
      "\n",
      "Epoch:  12\n",
      "Batch 50, Loss: 0.544\n",
      "Batch 100, Loss: 0.559\n",
      "Batch 150, Loss: 0.556\n",
      "TOTAL EPOCH LOSS: 0.10130220733117312\n",
      "Accuracy on the test set: 78.29%\n",
      "LR:  0.005\n",
      "\n",
      "Epoch:  13\n",
      "Batch 50, Loss: 0.538\n",
      "Batch 100, Loss: 0.532\n",
      "Batch 150, Loss: 0.539\n",
      "TOTAL EPOCH LOSS: 0.0976471253670752\n",
      "Accuracy on the test set: 78.28%\n",
      "LR:  0.005\n",
      "\n",
      "Epoch:  14\n",
      "Batch 50, Loss: 0.538\n",
      "Batch 100, Loss: 0.534\n",
      "Batch 150, Loss: 0.525\n",
      "TOTAL EPOCH LOSS: 0.09461167105473578\n",
      "Accuracy on the test set: 79.80%\n",
      "LR:  0.005\n",
      "\n",
      "Epoch:  15\n",
      "Batch 50, Loss: 0.498\n",
      "Batch 100, Loss: 0.517\n",
      "Batch 150, Loss: 0.520\n",
      "TOTAL EPOCH LOSS: 0.0951817388413474\n",
      "Accuracy on the test set: 78.95%\n",
      "LR:  0.005\n",
      "\n",
      "Epoch:  16\n",
      "Batch 50, Loss: 0.485\n",
      "Batch 100, Loss: 0.504\n",
      "Batch 150, Loss: 0.498\n",
      "TOTAL EPOCH LOSS: 0.09338456951081753\n",
      "Accuracy on the test set: 79.33%\n",
      "LR:  0.005\n",
      "\n",
      "Epoch:  17\n",
      "Batch 50, Loss: 0.481\n",
      "Batch 100, Loss: 0.490\n",
      "Batch 150, Loss: 0.506\n",
      "TOTAL EPOCH LOSS: 0.08814143436029553\n",
      "Accuracy on the test set: 80.31%\n",
      "LR:  0.005\n",
      "\n",
      "Epoch:  18\n",
      "Batch 50, Loss: 0.473\n",
      "Batch 100, Loss: 0.481\n",
      "Batch 150, Loss: 0.482\n",
      "TOTAL EPOCH LOSS: 0.08888898498844355\n",
      "Accuracy on the test set: 79.60%\n",
      "LR:  0.005\n",
      "\n",
      "Epoch:  19\n",
      "Batch 50, Loss: 0.467\n",
      "Batch 100, Loss: 0.461\n",
      "Batch 150, Loss: 0.474\n",
      "TOTAL EPOCH LOSS: 0.08545358933042735\n",
      "Accuracy on the test set: 79.49%\n",
      "LR:  0.005\n",
      "\n",
      "Epoch:  20\n",
      "Batch 50, Loss: 0.453\n",
      "Batch 100, Loss: 0.445\n",
      "Batch 150, Loss: 0.472\n",
      "TOTAL EPOCH LOSS: 0.08505441120360047\n",
      "Accuracy on the test set: 80.14%\n",
      "LR:  0.0025\n",
      "\n",
      "Epoch:  21\n",
      "Batch 50, Loss: 0.411\n",
      "Batch 100, Loss: 0.409\n",
      "Batch 150, Loss: 0.429\n",
      "TOTAL EPOCH LOSS: 0.07333372114226222\n",
      "Accuracy on the test set: 81.40%\n",
      "LR:  0.0025\n",
      "\n",
      "Epoch:  22\n",
      "Batch 50, Loss: 0.400\n",
      "Batch 100, Loss: 0.402\n",
      "Batch 150, Loss: 0.390\n",
      "TOTAL EPOCH LOSS: 0.07073909079190344\n",
      "Accuracy on the test set: 81.13%\n",
      "LR:  0.0025\n",
      "\n",
      "Epoch:  23\n",
      "Batch 50, Loss: 0.369\n",
      "Batch 100, Loss: 0.385\n",
      "Batch 150, Loss: 0.391\n",
      "TOTAL EPOCH LOSS: 0.07092289195861667\n",
      "Accuracy on the test set: 81.34%\n",
      "LR:  0.0025\n",
      "\n",
      "Epoch:  24\n",
      "Batch 50, Loss: 0.366\n",
      "Batch 100, Loss: 0.372\n",
      "Batch 150, Loss: 0.379\n",
      "TOTAL EPOCH LOSS: 0.06896044849418104\n",
      "Accuracy on the test set: 81.22%\n",
      "LR:  0.0025\n",
      "\n",
      "Epoch:  25\n",
      "Batch 50, Loss: 0.375\n",
      "Batch 100, Loss: 0.378\n",
      "Batch 150, Loss: 0.370\n",
      "TOTAL EPOCH LOSS: 0.06842828262597322\n",
      "Accuracy on the test set: 81.51%\n",
      "LR:  0.0025\n",
      "\n",
      "Epoch:  26\n",
      "Batch 50, Loss: 0.363\n",
      "Batch 100, Loss: 0.371\n",
      "Batch 150, Loss: 0.361\n",
      "TOTAL EPOCH LOSS: 0.06747602543327957\n",
      "Accuracy on the test set: 82.10%\n",
      "LR:  0.0025\n",
      "\n",
      "Epoch:  27\n",
      "Batch 50, Loss: 0.359\n",
      "Batch 100, Loss: 0.353\n",
      "Batch 150, Loss: 0.356\n",
      "TOTAL EPOCH LOSS: 0.0661394172348082\n",
      "Accuracy on the test set: 81.56%\n",
      "LR:  0.0025\n",
      "\n",
      "Epoch:  28\n",
      "Batch 50, Loss: 0.336\n",
      "Batch 100, Loss: 0.361\n",
      "Batch 150, Loss: 0.349\n",
      "TOTAL EPOCH LOSS: 0.06380363274365664\n",
      "Accuracy on the test set: 81.71%\n",
      "LR:  0.0025\n",
      "\n",
      "Epoch:  29\n",
      "Batch 50, Loss: 0.336\n",
      "Batch 100, Loss: 0.355\n",
      "Batch 150, Loss: 0.355\n",
      "TOTAL EPOCH LOSS: 0.0642898486694321\n",
      "Accuracy on the test set: 82.34%\n",
      "LR:  0.0025\n",
      "\n",
      "Epoch:  30\n",
      "Batch 50, Loss: 0.337\n",
      "Batch 100, Loss: 0.342\n",
      "Batch 150, Loss: 0.338\n",
      "TOTAL EPOCH LOSS: 0.06634413346182555\n",
      "Accuracy on the test set: 81.58%\n",
      "LR:  0.00125\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▂▄▅▅▅▆▆▆▇▇▇▇▇▇▇█▇▇███████████</td></tr><tr><td>LearningRate</td><td>█████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>Loss</td><td>█▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.8158</td></tr><tr><td>LearningRate</td><td>0.00125</td></tr><tr><td>Loss</td><td>0.06634</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-shape-23</strong> at: <a href='https://wandb.ai/marcospagnoletti-ms/ResNet19-CIFAR10-test/runs/8y4ipohk' target=\"_blank\">https://wandb.ai/marcospagnoletti-ms/ResNet19-CIFAR10-test/runs/8y4ipohk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231213_104308-8y4ipohk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 81.58%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8158"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "current_epoch = 0\n",
    "\n",
    "print(\"USING DEVICE: \",device)\n",
    "\n",
    "# initialize Step LR\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    current_epoch += 1\n",
    "    print(\"\")\n",
    "    print(\"Epoch: \",current_epoch)\n",
    "    #train_loop(tr_loader, model18, criterion, optimizer)\n",
    "    \n",
    "\n",
    "    Loss = train(model18, train_loader, criterion, optimizer, device)\n",
    "    Acc = test(model18, test_loader, device)\n",
    "    scheduler.step()\n",
    "    LearnRate = scheduler.get_last_lr()[0]\n",
    "    print(\"LR: \",LearnRate)\n",
    "    wandb.log({\"Accuracy\":Acc, \"Loss\": Loss, \"LearningRate\":LearnRate})\n",
    "    \n",
    "\n",
    "print('Finished Training')\n",
    "wandb.finish()\n",
    "\n",
    "# Testing the model\n",
    "test(model18, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
